{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3510)\n",
    "random_state = 3510\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[130612:].drop(\"qid\", axis=1)\n",
    "test = data[:130612].drop(\"qid\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "UNK = 1\n",
    "BOS = 2\n",
    "EOS = 3\n",
    "\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "BOS_TOKEN = '<S>'\n",
    "EOS_TOKEN = '</S>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {\n",
    "    PAD_TOKEN: PAD,\n",
    "    BOS_TOKEN: BOS,\n",
    "    EOS_TOKEN: EOS,\n",
    "    UNK_TOKEN: UNK,\n",
    "    }\n",
    "\n",
    "id2word = {v: k for k, v in word2id.items()}   \n",
    "\n",
    "for s in train[\"question_text\"]:\n",
    "    for w in s.split():\n",
    "        _id = len(word2id)\n",
    "        word2id.setdefault(w, _id)\n",
    "        id2word[_id] = w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_ids(char2id, sentence):\n",
    "    \"\"\"\n",
    "    単語のリストをインデックスのリストに変換する\n",
    "    :param vocab: Vocabのインスタンス\n",
    "    :param sentence: list of str\n",
    "    :return indices: list of int\n",
    "    \"\"\"\n",
    "    ids = [char2id.get(c, UNK) for c in sentence.split()]\n",
    "    ids = [BOS] + ids + [EOS]  # </S>トークンを末尾に加える\n",
    "#     ids += [EOS]  # EOSを末尾に加える\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"question_text\"]= [sentence_to_ids(word2id, sentence) for sentence in train[\"question_text\"]]\n",
    "test[\"question_text\"] = [sentence_to_ids(word2id, sentence) for sentence in test[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_seqs(seqs):\n",
    "            # パディング\n",
    "    max_length = max([len(s) for s in seqs])\n",
    "    data = [s + [PAD] * (max_length - len(s)) for s in seqs]\n",
    "     # テンソルに変換\n",
    "    data_tensor = torch.tensor(data, dtype=torch.long, device=device)\n",
    "    return data_tensor   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, data, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        :param src_insts: list, 入力言語の文章（単語IDのリスト）のリスト\n",
    "        :param tgt_insts: list, 出力言語の文章（単語IDのリスト）のリスト\n",
    "        :param batch_size: int, バッチサイズ\n",
    "        :param shuffle: bool, サンプルの順番をシャッフルするか否か\n",
    "        \"\"\"\n",
    "        self.positive = data[data.target==1]\n",
    "        self.negative = data[data.target==0]\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.start_index = 0\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        if self.shuffle:\n",
    "            self.negative = shuffle(self.negative, random_state=random_state)\n",
    "        self.start_index = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):       \n",
    "        # ポインタが最後まで到達したら初期化する\n",
    "        if self.start_index >= len(self.negative):\n",
    "            self.reset()\n",
    "            raise StopIteration()\n",
    "        \n",
    "        minibatch = pd.concat([self.positive.sample(self.batch_size), self.negative[self.start_index:self.start_index+self.batch_size]], axis=0)\n",
    "        \n",
    "        minibatch_X = preprocess_seqs(minibatch[\"question_text\"].values)\n",
    "        minibatch_y = torch.tensor(minibatch[\"target\"].values, dtype=torch.long, device=device)\n",
    "        # バッチを取得して前処理\n",
    "        self.start_index += self.batch_size\n",
    "\n",
    "        return minibatch_X, minibatch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, class_num, kernel_num, kernel_sizes, dropout, static):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 入力言語の語彙数\n",
    "        :param embedding_size: int, 埋め込みベクトルの次元数\n",
    "        :param class_num: int, 出力のクラス数\n",
    "        :param kernel_num: int,　畳み込み層の出力チャネル数\n",
    "        :param kernel_sizes: list of int, カーネルのウィンドウサイズ\n",
    "        :param dropout: float, ドロップアウト率\n",
    "        :param static: bool, 埋め込みを固定するか否かのフラグ\n",
    "        \"\"\"\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        self.static = static\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        # nn.ModuleList: 任意の数のModuleをlistのような形で保持することが出来るクラス\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv1d(1, kernel_num, (kernel_size, embedding_size)) for kernel_size in kernel_sizes]\n",
    "            )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(len(kernel_sizes)*kernel_num, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, max_length)\n",
    "        x = self.embedding(x)  # (batch_size, max_length, embedding_size)\n",
    "        \n",
    "        if self.static:\n",
    "            x = torch.tensor(x)  # 埋め込みを固定\n",
    "\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, max_length, embedding_size)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(batch_size, kernel_num, max_length-kernel_size+1), ...]*len(kernel_sizes)\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(batch_size, kernel_num), ...]*len(kernel_sizes)\n",
    "\n",
    "        x = torch.cat(x, 1)  # (batch_size, len(kernel_sizes)*kernel_num)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        logit = self.out(x)  # (batch_size, class_num)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'vocab_size': len(id2word),\n",
    "    'embedding_size': 128,\n",
    "    'class_num': 2,\n",
    "    'kernel_num': 64,\n",
    "    'kernel_sizes': [3, 4, 5],\n",
    "    'dropout': 0.5,\n",
    "    'static': False,\n",
    "}\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "ckpt_path = 'cnn.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = TextCNN(**model_args)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = preprocess_seqs(test[\"question_text\"].values)\n",
    "test_y = torch.tensor(test[\"target\"].values, dtype=torch.long, device=device)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train_loss=[]\n",
    "log_valid_loss=[]\n",
    "# 訓練\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = 0.\n",
    "    # train\n",
    "    for batch_X, batch_Y in train_dataloader:\n",
    "        print(\"training...\")\n",
    "        model.train()\n",
    "        pred_Y = model(batch_X)\n",
    "        loss = criterion(pred_Y, batch_Y.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        \n",
    "        train_loss += loss\n",
    "    train_loss = train_loss/len(train)\n",
    "    \n",
    "    pred_Y = model(test_X)\n",
    "    valid_loss = criterion(pred_Y, test_y.view(-1))\n",
    "    print(\"train:\",train_loss, \", valid:\", valid_loss)\n",
    "    log_train_loss.append(train_loss)\n",
    "    log_valid_loss.append(valid_loss)    \n",
    "    \n",
    "    ckpt = model.state_dict()\n",
    "    torch.save(ckpt, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log_train_loss)\n",
    "plt.plot(log_valid_loss)\n",
    "plt.savefig('cnn.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
